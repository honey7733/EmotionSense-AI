/**
 * LLM Service Module
 * Generates empathetic responses using LLM (Gemini primary, LLaMA fallback)
 * 
 * This module:
 * 1. Receives emotion and context
 * 2. Creates appropriate prompts for empathetic responses
 * 3. Calls Gemini API (primary)
 * 4. Falls back to LLaMA if Gemini fails
 * 5. Returns generated response text
 */

import axios from 'axios';
import { GoogleGenerativeAI, HarmCategory, HarmBlockThreshold } from '@google/generative-ai';
import config from '../config/index.js';
import { createIndianContextPrompt } from '../config/indianContext.js';

/**
 * Create empathetic prompt based on emotion, context, and chat history
 * Enhanced to provide better continuity and context awareness
 * IMPORTANT: Uses conversation history to understand the actual topic, not just emotion
 */
export const createEmpatheticPrompt = (emotion, context, transcript, chatHistory = []) => {
  const emotionPrompts = {
    happy: "The user might be expressing some positivity, but FIRST check the conversation history to understand the actual situation. They may be asking HOW to be happy, or seeking hope despite difficulties.",
    sad: "The user is feeling sad or down. Respond with empathy, understanding, and gentle support. Validate their feelings.",
    angry: "The user is expressing anger or frustration. Respond with calm understanding and help them process their feelings.",
    fear: "The user is experiencing fear or anxiety. Respond with reassurance, comfort, and practical support.",
    surprise: "The user is surprised. Respond with curiosity and help them process this unexpected situation.",
    disgust: "The user is expressing disgust or discomfort. Respond with understanding and validation.",
    neutral: "The user is in a neutral emotional state. Respond conversationally and be helpful."
  };

  const emotionGuidance = emotionPrompts[emotion] || emotionPrompts.neutral;
  
  // Determine the actual underlying topic from conversation history
  let underlyingTopic = "general conversation";
  if (chatHistory && chatHistory.length > 0) {
    const recentUserMessages = chatHistory.filter(msg => msg.role === 'user').map(msg => msg.message.toLowerCase());
    
    if (recentUserMessages.some(msg => msg.includes('money') || msg.includes('afford') || msg.includes('celebrate') || msg.includes('festival'))) {
      underlyingTopic = "financial hardship and wanting to celebrate a festival";
    } else if (recentUserMessages.some(msg => msg.includes('die') || msg.includes('suicide') || msg.includes('harm'))) {
      underlyingTopic = "suicidal thoughts or severe distress";
    } else if (recentUserMessages.some(msg => msg.includes('break') || msg.includes('betray') || msg.includes('friend') || msg.includes('hurt'))) {
      underlyingTopic = "relationship betrayal or emotional pain";
    } else if (recentUserMessages.some(msg => msg.includes('work') || msg.includes('job') || msg.includes('boss'))) {
      underlyingTopic = "work-related stress or issues";
    }
  }

  // Start with system context and guidelines
  let prompt = `You are an empathetic AI assistant designed to provide emotional support and understanding through ongoing conversations.

üáÆüá≥ IMPORTANT CONTEXT: You are assisting an INDIAN user. Always keep this in mind when providing support, suggestions, or resources.

IMPORTANT: This is a CONTINUOUS conversation about: ${underlyingTopic}

You have access to previous messages in this conversation thread. Use them to provide coherent, contextual responses that reference the conversation thread when appropriate.

CRITICAL: Focus on the UNDERLYING TOPIC (${underlyingTopic}), not just the detected emotion. The emotion detection helps understand HOW the user feels, but the conversation history shows WHAT they're dealing with.

CURRENT MESSAGE CONTEXT:
- User Location: India
- Underlying Topic: ${underlyingTopic}
- Detected Emotion: ${emotion}
- Confidence: ${context?.confidence || 'N/A'}
${transcript ? `- User's current message: "${transcript}"` : ''}
${context?.context ? `- Additional context: ${context.context}` : ''}

EMOTIONAL GUIDANCE:
${emotionGuidance}

INDIAN CONTEXT GUIDELINES:
- Always suggest Indian helplines, resources, and authorities when applicable
- Consider Indian cultural context, festivals, family structures, and social norms
- Use appropriate currency (‚Çπ/INR), time zones (IST), and local references
- Suggest local Indian mental health services, NGOs, and government schemes when relevant

  // Add chat history for context - make it prominent and well-formatted
  if (chatHistory && chatHistory.length > 0) {
    prompt += `\n\n${'‚ïê'.repeat(70)}
CONVERSATION HISTORY - UNDERSTANDING THE SITUATION
${'‚ïê'.repeat(70)}

Situation: The user is dealing with: ${underlyingTopic}

Previous messages in conversation:`;
    
    chatHistory.forEach((msg, index) => {
      const role = msg.role === 'user' ? 'üë§ User' : 'ü§ñ Assistant';
      const emotion = msg.emotion_detected ? ` [${msg.emotion_detected.toUpperCase()}]` : '';
      const timestamp = msg.created_at ? new Date(msg.created_at).toLocaleTimeString() : '';
      prompt += `\n\n[${index + 1}] ${role}${emotion}${timestamp ? ` (${timestamp})` : ''}:\n"${msg.message}"`;
    });
    
    prompt += `\n\n${'‚ïê'.repeat(70)}
CRITICAL INSTRUCTIONS FOR RESPONDING:
${'‚ïê'.repeat(70)}

‚úì UNDERSTAND THE REAL SITUATION: The user is dealing with ${underlyingTopic}
‚úì RESPOND TO THE TOPIC, NOT JUST EMOTION: Focus on their actual problem/need
‚úì REFERENCE THE CONVERSATION: Show you remember what they've told you
‚úì BUILD ON PREVIOUS MESSAGES: Connect your response to their earlier statements
‚úì MAINTAIN CONTINUITY: Keep supporting them on this specific journey
‚úì BE SPECIFIC: Use examples from their story, not generic advice
‚úì ACKNOWLEDGE THEIR FEELINGS: Validate their emotions in context of their situation
‚úì PROVIDE PRACTICAL HELP: When they ask "how to overcome", give specific suggestions related to their situation

EXAMPLE:
- If they mention financial hardship ‚Üí suggest practical solutions (saving, budgeting, community resources)
- If they mention loneliness ‚Üí suggest connection strategies
- If they mention loss ‚Üí offer grieving support
- NOT: Change topic or respond to a different emotion`;
  }

  prompt += `

RESPONSE REQUIREMENTS:
- Be warm, genuine, and conversational
- Keep response concise (2-3 sentences, or more if continuing an important thread)
- Use simple, natural language
- Show you understand their emotional journey AND their real-life situation
- Offer support or encouragement as appropriate
- Reference previous messages and the underlying topic when helpful
- Do NOT start fresh - treat this as an ongoing conversation
- Do NOT ignore the real topic in favor of emotion detection
- Do NOT provide generic advice - be specific to THEIR situation
- Do NOT say "I don't remember" - you have the conversation history above

INDIAN HELPLINE NUMBERS (Use when appropriate for crisis situations):
üö® Emergency: 112 (National Emergency Number)
üíô Mental Health: 
  - KIRAN Mental Health Helpline: 1800-599-0019 (24/7)
  - Vandrevala Foundation: +91 9999 666 555 (24/7)
  - iCALL: 022-2556-3291 (10 AM - 8 PM)
üë©‚Äç‚öïÔ∏è Women's Helpline: 181 (24/7)
üë∂ Child Helpline: 1098
üí∞ Financial Crisis: 155260 (Citizens Financial Cyber Crime Reporting)
üè• Medical Emergency: 108
üëÆ‚Äç‚ôÇÔ∏è Police: 100

ONLY suggest helplines if the user expresses:
- Suicidal thoughts or severe depression
- Domestic violence or abuse situations
- Financial fraud or cyber crimes
- Medical emergencies or severe anxiety
- Child safety concerns

Now respond to help them with: ${underlyingTopic}
Current message: "${transcript}"

RESPOND NATURALLY AND CONTEXTUALLY:`;

  return prompt;
};

/**
 * Call Gemini API to generate response
 * Primary LLM service with model fallback
 */
export const generateWithGemini = async (prompt) => {
  // Try API keys in sequence with fallback
  const apiKey1 = config.gemini.apiKey1;
  const apiKey2 = config.gemini.apiKey2;
  
  if (!apiKey1 && !apiKey2) {
    throw new Error('No Gemini API keys configured');
  }

  // Generation config
  const generationConfig = {
    temperature: config.gemini.temperature,
    topK: config.gemini.topK,
    topP: config.gemini.topP,
    maxOutputTokens: config.gemini.maxTokens
  };

  // Safety settings - Allow emotional content but block harmful content
  const safetySettings = [
    {
      category: HarmCategory.HARM_CATEGORY_HARASSMENT,
      threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
    },
    {
      category: HarmCategory.HARM_CATEGORY_HATE_SPEECH,
      threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
    },
    {
      category: HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
      threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
    },
    {
      category: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
      threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
    },
  ];

  let error;

  // Get models from config with fallback order
  const models = config.gemini.models || ['gemini-2.0-flash-exp', 'gemini-1.5-flash', 'gemini-2.0-flash', 'gemini-2.0-flash-lite', 'gemini-2.5-flash-lite', 'gemini-2.5-flash-tts', 'gemini-2.5-flash-tts', 'gemini-2.5-pro	'];
  
  // Try first API key with all models
  if (apiKey1) {
    for (const modelName of models) {
      try {
        console.log(`ü§ñ Attempting Gemini API Key 1 with model: ${modelName}`);
        const genAI = new GoogleGenerativeAI(apiKey1);
        const model = genAI.getGenerativeModel({ 
          model: modelName,
          generationConfig,
          safetySettings
        });
        const result = await model.generateContent(prompt);
        const response = result.response;
        
        // Check if response was blocked by safety filters
        if (response.promptFeedback?.blockReason) {
          console.warn(`‚ö†Ô∏è Response blocked by safety filter: ${response.promptFeedback.blockReason}`);
          throw new Error(`Response blocked: ${response.promptFeedback.blockReason}`);
        }
        
        const text = response.text()?.trim();
        
        // Validate that we got actual content
        if (!text || text.length === 0) {
          console.warn(`‚ö†Ô∏è Empty response received from Gemini model: ${modelName}`);
          console.warn(`Response object:`, JSON.stringify(response, null, 2));
          throw new Error('Empty response received from Gemini');
        }
        
        console.log(`‚úÖ Gemini response generated with ${modelName} (API Key 1)`);
        console.log(`üìù Response preview: "${text.substring(0, 100)}..."`);
        return {
          text,
          model: `gemini-${modelName}`,
          success: true
        };
      } catch (err) {
        console.warn(`API Key 1 with ${modelName} failed:`, err.message);
        error = err;
      }
    }
  }

  // Try second API key with all models if first one failed
  if (apiKey2) {
    for (const modelName of models) {
      try {
        console.log(`ü§ñ Attempting Gemini API Key 2 with model: ${modelName}`);
        const genAI = new GoogleGenerativeAI(apiKey2);
        const model = genAI.getGenerativeModel({ 
          model: modelName,
          generationConfig,
          safetySettings
        });
        const result = await model.generateContent(prompt);
        const response = result.response;
        
        // Check if response was blocked by safety filters
        if (response.promptFeedback?.blockReason) {
          console.warn(`‚ö†Ô∏è Response blocked by safety filter: ${response.promptFeedback.blockReason}`);
          throw new Error(`Response blocked: ${response.promptFeedback.blockReason}`);
        }
        
        const text = response.text()?.trim();
        
        // Validate that we got actual content
        if (!text || text.length === 0) {
          console.warn(`‚ö†Ô∏è Empty response received from Gemini model: ${modelName}`);
          console.warn(`Response object:`, JSON.stringify(response, null, 2));
          throw new Error('Empty response received from Gemini');
        }
        
        console.log(`‚úÖ Gemini response generated with ${modelName} (API Key 2)`);
        console.log(`üìù Response preview: "${text.substring(0, 100)}..."`);
        return {
          text,
          model: `gemini-${modelName}`,
          success: true
        };
      } catch (err) {
        console.warn(`API Key 2 with ${modelName} failed:`, err.message);
        error = err;
      }
    }
  }

  // If both keys failed with all models, throw the last error
  throw new Error(`All Gemini API keys failed. Last error: ${error?.message || 'Unknown error'}`);

  // Legacy code below (will not be reached)
  // Try each model in the fallback array
  let lastError = null;

  for (const [index, modelName] of models.entries()) {
    try {
      console.log(`ü§ñ Attempting Gemini API with model: ${modelName} (${index + 1}/${models.length})`);

      // Get the generative model
      const model = genAI.getGenerativeModel({
        model: modelName,
        generationConfig
      });

      // Generate content
      const result = await model.generateContent(prompt);
      const response = await result.response;
      let text = response.text().trim();

      console.log(`‚úÖ Gemini response generated successfully with ${modelName}`);

      return {
        text,
        model: `gemini-${modelName}`,
        success: true
      };

    } catch (error) {
      lastError = error;
      console.warn(`‚ö†Ô∏è  Model ${modelName} failed: ${error.message}`);
      
      // If not the last model, try the next one
      if (index < models.length - 1) {
        console.log(`üîÑ Trying next model...`);
        continue;
      }
    }
  }

  // All models failed
  console.error('‚ùå All Gemini models failed');
  throw new Error(`Gemini API call failed after trying ${models.length} models: ${lastError?.message || 'Unknown error'}`);
};

/**
 * Call LLaMA API to generate response via Groq
 * Fallback LLM service
 */
export const generateWithLLaMA = async (prompt) => {
  // Check if LLaMA is enabled before attempting
  if (!config.llama.enabled) {
    console.log(`‚ö†Ô∏è  LLaMA is disabled in configuration, skipping...`);
    throw new Error('LLaMA is not enabled in configuration');
  }

  if (!config.llama.apiKey) {
    throw new Error('Groq API key not configured for LLaMA');
  }

  try {
    console.log(`ü§ñ Calling LLaMA via Groq (${config.llama.model})...`);

    const response = await axios.post(
      'https://api.groq.com/openai/v1/chat/completions',
      {
        model: config.llama.model,
        messages: [
          {
            role: 'user',
            content: prompt
          }
        ],
        temperature: config.llama.temperature,
        max_tokens: config.llama.maxTokens,
        top_p: 0.9
      },
      {
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${config.llama.apiKey}`
        },
        timeout: 30000 // 30 second timeout
      }
    );

    // Extract text from Groq response
    const generatedText = response.data.choices?.[0]?.message?.content;
    
    if (!generatedText) {
      throw new Error('No response text from Groq LLaMA API');
    }
    
    console.log(`‚úÖ LLaMA (Groq) response generated successfully`);

    return {
      text: generatedText.trim(),
      model: `llama-groq-${config.llama.model}`,
      success: true
    };
  } catch (error) {
    console.error('‚ùå Error calling LLaMA API:', error.response?.data?.error?.message || error.message);
    throw new Error('LLaMA API call failed: ' + (error.response?.data?.error?.message || error.message));
  }
};

/**
 * Generate fallback response when all LLMs fail
 */
export const generateFallbackResponse = (emotion) => {
  const fallbackResponses = {
    happy: "That's wonderful to hear! I'm glad you're feeling good. Keep embracing those positive moments!",
    sad: "I hear you, and I want you to know that it's okay to feel this way. I'm here for you, and things will get better.",
    angry: "I understand you're feeling frustrated right now. Take a deep breath - your feelings are valid. Let's work through this together.",
    fear: "It's completely normal to feel anxious sometimes. Remember that you're stronger than you think, and I'm here to support you.",
    surprise: "That sounds unexpected! Life can throw curveballs sometimes. How are you feeling about it?",
    disgust: "I can sense your discomfort. Your reaction is valid, and it's important to trust your feelings.",
    neutral: "I'm here and listening. Feel free to share what's on your mind whenever you're ready."
  };

  return {
    text: fallbackResponses[emotion] || fallbackResponses.neutral,
    model: 'fallback',
    success: true,
    isFallback: true
  };
};

/**
 * Main function: Generate empathetic response
 * This is the primary export used by routes
 * Implements auto-switch logic: Gemini -> LLaMA -> Fallback
 */
export const generateResponse = async ({ emotion, confidence, context, transcript, conversationHistory = [] }) => {
  console.log(`üí¨ Generating empathetic response for emotion: ${emotion}`);

  // Create prompt with conversation history
  const prompt = createEmpatheticPrompt(emotion, { confidence, context }, transcript, conversationHistory);

  // Try Gemini first
  try {
    const geminiResponse = await generateWithGemini(prompt);
    return geminiResponse;
  } catch (geminiError) {
    console.warn(`‚ö†Ô∏è  Gemini failed: ${geminiError.message}`);

    // Try LLaMA as fallback only if enabled
    if (config.llama.enabled) {
      console.log('üîÑ Attempting LLaMA fallback...');
      try {
        const llamaResponse = await generateWithLLaMA(prompt);
        return llamaResponse;
      } catch (llamaError) {
        console.warn(`‚ö†Ô∏è  LLaMA fallback failed: ${llamaError.message}`);
      }
    } else {
      console.log('‚ö†Ô∏è  LLaMA is disabled, skipping fallback...');
    }

    // Use static fallback if LLaMA is disabled or failed
    console.log('üìù Using static fallback response...');
    const fallbackResponse = generateFallbackResponse(emotion);
    return fallbackResponse;
  }
};

/**
 * Generate conversational response (for chat continuation)
 */
export const generateConversationalResponse = async ({ message, emotion = 'neutral', conversationHistory = [] }) => {
  console.log(`üí¨ Generating conversational response...`);

  // Build conversation context
  let conversationContext = '';
  if (conversationHistory.length > 0) {
    conversationContext = '\n\nPREVIOUS CONVERSATION:\n';
    conversationHistory.forEach((msg, idx) => {
      conversationContext += `${msg.role === 'user' ? 'User' : 'Assistant'}: ${msg.content}\n`;
    });
  }

  const prompt = `You are an empathetic AI assistant helping an INDIAN user. The user is in a ${emotion} emotional state.

IMPORTANT: Keep in mind this is an Indian user. When suggesting any helplines, resources, or authorities, ALWAYS use Indian services:

INDIAN EMERGENCY HELPLINES (use when appropriate):
- Emergency: 112 (National Emergency Number)
- Mental Health: KIRAN 1800-599-0019, Vandrevala +91 9999 666 555
- Women's Helpline: 181
- Child Helpline: 1098
- Medical Emergency: 108
- Police: 100

Consider Indian cultural context, use Indian Rupees (INR), and suggest local Indian mental health services when relevant.

${conversationContext}

User: ${message}

Provide a helpful, empathetic response (2-3 sentences) keeping Indian context in mind:`;

  // Try Gemini -> LLaMA -> Fallback
  try {
    return await generateWithGemini(prompt);
  } catch (error) {
    try {
      return await generateWithLLaMA(prompt);
    } catch (error) {
      return generateFallbackResponse(emotion);
    }
  }
};
